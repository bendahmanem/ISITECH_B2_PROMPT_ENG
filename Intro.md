# Introduction a l'IA

L'histoire de l'intelligence débute peu après la seconde guerre mondiale... Et ce terme apparait pour la première fois en 1956 lors d'une conférence à Dartmouth College.

Les premiers travaux d'IA consistaient a creer des programmes dont le but etait de prouver des theoremes mathematiques.
Et c'est dans les années 60 que le prmier chatbot fu crée, le robot Shakey.

![alt text](image.png)
<u>Le robot Shakey</u>

Shakey, fut principalement utilisé pour tester des algorithmes de planification et de raisonnement et des taches simples en laboratioire.

Des la fin des années des 60, on annonce que l'IA est sur le point de résoudre tous les problèmes de l'humanité.

## Les systemes experts

Prenons l'exemple d'un systeme expert chargé de realtion client:

- Relge 1 : si le client pose une question sur les horaires d'ouverture, alors on lui répondra les horaires d'ouverture.
- Regle 2: si le cient fait mention du terme "probleme", lui offir de l'aide en le redirigeant vers le service concerné.
- si le client demande un remboursement, lui demander son numero de commande.

Ces systemes experts sont basés sur des regles de logique et de raisonnement.

Voici le schéma de fonctionnement d'un systeme expert:

![alt text](image-1.png)

On arrive avec ce genre de systeme vers des projets comme Deep Blue qui a battu le champion du monde d'echec en 1997.

## Les dates clés de l'IA :

1950 : Alan Turing publie son essai "Computing Machinery and Intelligence", posant ainsi les fondements du débat sur la capacité des machines à penser, et introduisant le célèbre Test de Turing.

1956 : la conférence de Dartmouth College marque le point de départ officiel de la recherche en intelligence artificielle en tant que discipline distincte.

1959 : Allen Newell et Herbert A. Simon développent Logic Theorist, un programme pionnier dans le domaine de l’intelligence artificielle.

1966 : Joseph Weizenbaum crée ELIZA, un programme de traitement du langage naturel simulant des conversations avec un psychothérapeute.

1969 : Terry Winograd conçoit SHRDLU, un programme démontrant la capacité d’une machine à comprendre et manipuler des objets dans un environnement virtuel.

Années 1980 : l’intelligence artificielle traverse une période de ralentissement, caractérisée par un financement limité et un intérêt en baisse de la part du public et des médias en raison de résultats décevants.

1997 : le superordinateur Deep Blue d’IBM bat le champion du monde d’échecs Garry Kasparov, marquant un tournant dans la perception de l’IA.

2001 : le film A.I. Intelligence Artificielle de Steven Spielberg explore les implications émotionnelles de l’intelligence artificielle.

2011 : le programme IBM Watson remporte le jeu télévisé Jeopardy !, démontrant des compétences en traitement du langage naturel et en recherche d’informations.

2012 : l’algorithme de deep learning AlexNet remporte le défi ImageNet, inaugurant l’utilisation généralisée des réseaux neuronaux profonds en vision par ordinateur.

2014 : Facebook développe DeepFace, un système d’IA capable d’une reconnaissance faciale humaine de haute précision.

2016 : le programme AlphaGo, créé par DeepMind (filiale de Google), défait le champion mondial du jeu de go, marquant un progrès majeur dans la résolution de problèmes complexes.

2017 : l’avènement des réseaux neuronaux génératifs (GAN) permet la création d’images et de contenus authentiques.

2018 : la société OpenAI dévoile GPT (Generative Pre-trained Transformer), représentant un saut en avant dans le traitement du langage naturel.

2019 : DeepMind lance AlphaStar, une IA surpassant des joueurs humains professionnels à StarCraft II.

2020 : OpenAI présente GPT-3, la dernière itération de la série, dotée de compétences de traitement du langage naturel encore plus avancées.

2021 : la Chine lance le projet "Made in China 2025", prévoyant d’importants investissements dans l’IA.

2021 : la FDA (Food and Drug Administration, l’agence nationale du médicament américain) approuve le premier système d’IA pour l’imagerie médicale, capable d’aider au diagnostic du cancer du sein.

## L'IA Generative

L’intelligence artificielle a évolué bien au-delà de simples programmes exécutant des tâches prédictibles. L’une des avancées les plus intrigantes et captivantes de l’IA est sa capacité à créer, à imaginer et à générer des contenus originaux. Cette facette fascinante est appelée : « IA générative ».

L’IA générative ouvre de fait de nouvelles perspectives dans l’innovation, dans l’art, dans le divertissement et même dans la résolution de problèmes complexes.

Elle repose sur des réseaux neuronaux d’apprentissage profond, tels que les réseaux génératifs adverses, en français, que vous retrouverez également en anglais sous son acronyme GAN (Generative Adversarial Networks), ainsi que les Transformers que nous aborderons au fur et à mesure dans cet ouvrage. Ces réseaux sont entraînés à partir d’énormes volumes de données et une fois entraînés, ils peuvent générer des données nouvelles et originales qui ressemblent à celles sur lesquelles ils ont été formés.

L’IA générative peut également créer des images réalistes, des compositions musicales et même des textes cohérents. Les GAN (réseaux génératifs adverses) sont célèbres pour leur capacité à générer des images qui semblent être le produit d’un artiste humain, mais qui n’existent en réalité que dans le code informatique.

Les Transformers, quant à eux, sont excellents pour générer des morceaux de texte, qu’il s’agisse de poèmes, de scripts de films ou d’articles de blog, nous y reviendrons tout au long du chapitre DALL-E : exploiter la créativité de l’IA.

Avec tout cela, notre IA dite générative dispose de champs d’applications vastes et variés. Dans le monde de l’art, elle peut créer des œuvres visuelles et musicales inédites, ouvrant de nouvelles voies créatives pour les artistes et vraisemblablement de nouveaux métiers... Dans le domaine du divertissement, elle peut, par exemple être utilisée pour générer des personnages et des scénarios pour les jeux vidéo. Enfin, dans la conception de produits, l’IA générative peut proposer des concepts innovants en fonction des critères émis en entrée.

L’IA générative ne doit pas être perçue comme une menace pour la créativité humaine, mais plutôt comme un outil collaboratif, parfois même un assistant. Les artistes peuvent utiliser l’IA comme source d’inspiration ou de génération de concepts originaux, avant d’ajouter une touche un peu plus personnelle. Cette combinaison de l’intelligence humaine et artificielle devrait aboutir à des œuvres innovantes et stimulantes qui révolutionneront bon nombre d’industries.

En comprenant le potentiel et les limites de l’IA générative, nous pouvons dès à présent explorer comment elle transforme notre compréhension de la créativité, redéfinissant ce que signifie être un artiste et comment les machines peuvent participer à l’acte créatif. L’IA générative n’est pas simplement une manifestation technologique, mais un tremplin pour de nouvelles idées et de nouvelles perspectives dans le monde de la création qui est finalement infini.

## Classification des IA

La classification de l’intelligence artificielle a pour vocation de permettre la compréhension des différentes catégories de systèmes développés dans ce domaine. Nous vous proposons ci-dessous les quatre domaines de classification dont vous devez avoir connaissance.

### L'intelligence artificielle faible

L’IA faible, également connue sous les noms et acronymes : « d’IA étroite », de « Narrow Ai » ou bien encore « ANI », fait référence à des systèmes préalablement conçus dans le but de mener à bien une tâche précise et de manière intelligente. Elle est en fait considérée comme étant monotâche. L’IA faible se définie par la simulation d’un comportement humain, tout autant que par la restriction qu’on lui impose avec des contraintes se limitant à la tâche à accomplir. Elle est tout bonnement experte en son domaine de prédilection.

Malgré leurs apparences, ces systèmes sont donc plutôt limités dans leurs capacités et ne peuvent pas généraliser leur apprentissage à d’autres domaines.

Quelques exemples courants d’IA faible :

- les systèmes de recommandation ;

- les **Chatbots** ;

- les moteurs de recherche ;

- les voitures à conduite autonome ;

- les outils de cartographie ;

- les assistants virtuels ;

- les systèmes de reconnaissance faciale ;

- Siri, l’assistant virtuel Apple.

Comprenez par là, qu’une l’IA faible est orientée vers un objectif unique, bien précis, consistant à accomplir des tâches dédiées et spécifiques.

### L'intelligence artificielle forte

Cette dernière est aussi connue sous les noms et acronymes de IA profonde, ou en anglais AGI pour Artificial General Intelligence. À la différence de l’IA faible, l’IA forte vise à imiter l’intelligence humaine à tous les niveaux.

Ces systèmes sont capables de comprendre, d’apprendre, de raisonner et d’effectuer des tâches complexes de manière similaire à un être humain. L’idée ici, c’est de développer tout un système d’apprentissage (capacité cognitive) permettant à la machine de s’améliorer continuellement et d’apprendre de ses erreurs en permanence. Elle est en quelque sorte supposée apporter une réponse à tout type de problème.

Quelques exemples courants d’IA forte :

- La cybersécurité en déjouant des attaques toujours nouvelles sur des failles auparavant jamais exploitées.

- La prédiction des comportements ou comment anticiper les mouvements boursiers ou bien encore météorologiques.

La programmation informatique.

La vérification de faits.

Nous allons sous doute vous décevoir, mais l’IA forte est encore à ce jour, un objectif à moyen/long terme. La communauté de chercheur n’a pas encore pu pleinement atteindre cet objectif en concevant une intelligence artificielle forte, consciente et robuste. Pour atteindre ce plafond de verre, le défi réside dans la nécessité de permettre aux machines de bénéficier d’une conscience leur octroyant la possibilité de discerner les émotions et les croyances humaines.

Cela suscite encore, à date, de bien nombreux débats sur ses implications éthiques et philosophiques de nos futurs systèmes.

### L'I.A. symbolique

Cette troisième typologie d’IA repose sur la manipulation de symboles lisibles par l’homme pour représenter leurs propres comportements et expertises.

C’est en fait un mécanisme d’abstraction qui repose sur des faits et règles et représentant des connaissances expertes. Cela ne vous rappelle rien ?

Ces systèmes utilisent des bases de connaissances structurées et des algorithmes de raisonnement pour représenter la connaissance et prendre des décisions que l’on pourrait penser intelligentes. L’IA symbolique a ainsi été largement utilisée dans les systèmes experts, qui sont conçus pour résoudre des problèmes spécifiques dans des domaines tels que la médecine, le droit et l’ingénierie.

Quelques exemples courants d’IA symbolique :

- les systèmes experts ;

- les systèmes à base de règles précises ;

- les systèmes à base de connaissances.

### L'I.A. connexionniste

Quatrième domaine de classification, l’intelligence artificielle dite connexionniste qui est également connue sous le nom de réseaux de neurones artificiels.

Sa source d’inspiration n’est rien d’autre, soyons modestes que le fonctionnement du cerveau humain. Elle vise essentiellement à reproduire des phénomènes mentaux ce qui n’est pas le cas de l’IA symbolique que nous avons pu voir précédemment.

Ces systèmes utilisent des réseaux de neurones que l’on dit interconnectés pour effectuer des tâches d’apprentissage et de reconnaissance.

Techniquement ces réseaux de neurones artificiels sont constitués de nœuds appelés neurones qui sont reliés entre eux par des connexions pondérées (synapses) qui permettent aux signaux d’être transmis à travers le réseau.

Chaque neurone reçoit des signaux d’entrée, effectue des calculs sur ces signaux et transmet un signal de sortie aux neurones suivants. L’apprentissage des réseaux de neurones se fait par ajustement des poids des connexions, permettant ainsi au réseau de s’adapter aux données et de générer des prédictions ou des classifications précises.

Les réseaux de neurones sont fréquemment employés dans des domaines tels que le traitement du langage naturel (NLP ou Natural Language Processing en anglais), la vision par ordinateur (OCR ou Optical Character Recognition en anglais), et la reconnaissance vocale. Ces sujets seront abordés dès le chapitre Les applications de l’IA de cet ouvrage.

Les réseaux de neurones sont largement utilisés dans des domaines tels que le traitement du langage naturel (NLP ou Natural Language Processing), la vision par ordinateur (OCR ou Optical Character Recognition) et la reconnaissance vocale que nous aborderons plus tard dans cet ouvrage.

## Les modèles de l'IA et le machine learning

Les modèles d'IA :

![alt text](image-2.png)

Nous allons explorer les différents modèles d’intelligence artificielle, à commencer par le machine learning (ML) et ses algorithmes, véritable pierre angulaire de l’IA.

Nous aborderons ensuite le traitement du langage naturel (NLP) offrant à nos machines la capacité à comprendre le langage humain avant d’aborder le « deep learning » (DL) et la Vision par Ordinateur.

### L'apprentissage automatique (Machine Learning)

![alt text](image-3.png)

Le machine learning, abrégé ML, représente l’une des branches les plus captivantes de l’intelligence artificielle, un sujet qui pourrait remplir un livre entier à lui seul. Cette approche technologique révolutionnaire permet aux experts en données, communément appelés « Data Scientists », d’alimenter les algorithmes avec des ensembles de données, donnant ainsi aux machines la capacité d’apprendre et de prendre des décisions sans nécessiter de programmation informatique préalable.

En d’autres termes, au lieu de suivre des instructions strictes, les machines peuvent maintenant acquérir des connaissances à partir de données numériques préexistantes et s’améliorer au fil du temps grâce à un apprentissage continu et à l’augmentation du volume de données disponibles.

Le machine learning sert principalement à repérer des tendances et des similitudes, souvent désignées sous le terme de "patterns", qu’il s’agisse d’images, de mots, de statistiques, ou autres.

L’objectif sous-jacent est l’automatisation des tâches et la capacité à effectuer des prédictions en se basant sur des données passées.

Quelques exemples d’utilisation du ML :

- La santé : détection des maladies.

- Le secteur industriel : permettant de surveiller les capteurs et les défaillances.

- Le commerce : tendances de vente.

- Le tourisme : tarifications selon des tendances.

Nous aborderons trois termes que nous appellerons paradigmes d’apprentissage du machine learning :

- **L’Apprentissage Supervisé** (Supervised Learning).

- **L’Apprentissage Non-Supervisé** (Unsupervised Learning).

- **L’Apprentissage par Renforcement** (Reinforcement Learning).

![alt text](image-4.png)

<u>Figure: **Les grands paradigmes de l’apprentissage machine**</u>

### L'apprentissage supervisé

L’apprentissage supervisé est une approche visant à créer des modèles « prédictifs » pour anticiper par exemple, les besoins d’une entreprise. Dans ce contexte, la machine dispose déjà des réponses attendues, et notre rôle consiste à guider son acquisition de connaissances en lui fournissant des exemples de questions et de réponses.

Prenons un exemple concret afin d’illustrer le concept de l’apprentissage supervisé. Imaginons qu’un Data Scientist souhaite entraîner un algorithme à déterminer si une image contient un panda, un ours ou un pingouin. Dans le cadre de l’apprentissage supervisé, il attribue une « étiquette » à chaque image de l’ensemble de données d’entraînement, précisant si l’image contient l’un de ces animaux ou non.

![alt text](image-5.png)

<u>Figure: **L'apprentissage supervisé illustré**</u>

L’efficacité du modèle est évaluée en comparant ses prédictions avec les valeurs réelles, souvent en utilisant des métriques telles que l’exactitude, la précision, le rappel, etc.

### L'apprentissage non-supervisé

À l’instar de l’apprentissage supervisé, l’apprentissage non supervisé est entraîné sur des données qui ne disposent pas d’étiquette (un panda n’est pas un panda, un canard n’est pas encore un canard...). Notre modèle d’apprentissage va se baser cette fois-ci, sur des similitudes, en regroupant ces dernières par ensembles et sous-ensembles que nous appellerons « Clustering ».

![alt text](image-6.png)

<u>Figure: **L'apprentissage non-supervisé illustré**</u>
